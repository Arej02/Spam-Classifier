import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pylab as plt


df=pd.read_csv('spam.csv',encoding="latin-1")


df1=df.copy()


df1.shape


df1.sample(5)


df1.info()





df1.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True,axis=1)


df1=df1.rename(columns={'v1':'Type','v2':'Text'})


from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()


df1.Type=le.fit_transform(df1.Type)


df1.sample(5)


df1.isnull().sum()


def delete_duplicate_values(df):
    print('Deleting the duplicated values:')
    num=df.duplicated().sum()
    if num>0:
        print(f'There are {num} duplicate values in your dataset.')
        df.drop_duplicates(keep='first',inplace=True)
        print(f'{num} duplicate values have been deleted.')
    else:
        print('There are no duplicate values.')


delete_duplicate_values(df1)





df1.Type.value_counts()


plt.figure(figsize=(6,6))
colors = ['Red', 'Blue']
explode = (0.05, 0.1)

plt.pie(df1.Type.value_counts(),autopct='%0.2f',labels=['ham','spam'],explode=explode,wedgeprops={'edgecolor': 'black'},colors=colors,textprops={'fontsize': 12},startangle=90)
plt.title('Pie Chart of Type of text messages',fontsize=14)
plt.show()





import nltk


#Finding the number of characters in the document:
df1['num_characters'] = df1['Text'].apply(len)


#nltk.download('punkt')
#nltk.download('punkt_tab')
df1['num_words']=df1.Text.apply(lambda x:len(nltk.word_tokenize(x)))


df1['num_sentences']=df1.Text.apply(lambda x:len(nltk.sent_tokenize(x)))


df1.sample(5)


df1[['num_characters','num_words','num_sentences']].describe().T





pd.set_option('display.max_colwidth', None)


df1[df1.num_words==220]





df1.drop(index=1578,inplace=True)


df1[['num_characters','num_words','num_sentences']].describe().T


df1[df1.num_words==196]





df1[df1.Type==0][['num_characters','num_words','num_sentences']].describe().T


df1[df1.Type==1][['num_characters','num_words','num_sentences']].describe().T





sns.histplot(df1[df1.Type==0]['num_characters'],color='green',kde=True,label='Not Spam')
sns.histplot(df1[df1.Type==1]['num_characters'],color='red',kde=True,label='Spam')
plt.legend(title='Message Type')
plt.title('Distribution of Number of Characters for Spam vs. Non-Spam Messages')
plt.show()


ham_skewness=df1[df1.Type==0]['num_characters'].skew()
spam_skewness=df1[df1.Type==1]['num_characters'].skew()


print(f' The skewness of characters in not spam messages is: {ham_skewness}')
print(f' The skewness of characters in spam messages is: {spam_skewness}')


sns.histplot(df1[df1.Type==0]['num_words'],color='purple',kde=True,label='Not Spam')
sns.histplot(df1[df1.Type==1]['num_words'],color='blue',kde=True,label='Spam')
plt.legend(title='Message Type')
plt.title('Distribution of Number of Words for Spam vs. Non-Spam Messages')
plt.show()


ham_skewness=df1[df1.Type==0]['num_words'].skew()
spam_skewness=df1[df1.Type==1]['num_words'].skew()


print(f' The skewness of words in not spam messages is:{ham_skewness}')
print(f' The skewness of words in spam messages is: {spam_skewness}')


sns.histplot(df1[df1.Type==0]['num_sentences'],color='yellow',kde=True,label='Not Spam')
sns.histplot(df1[df1.Type==1]['num_sentences'],color='black',kde=True,label='Spam')
plt.legend(title='Message Type')
plt.title('Distribution of Number of Sentences for Spam vs. Non-Spam Messages')
plt.show()


ham_skewness=df1[df1.Type==0]['num_sentences'].skew()
spam_skewness=df1[df1.Type==1]['num_sentences'].skew()


print(f' The skewness of sentences in not spam messages is: {ham_skewness}')
print(f' The skewness of sentences in spam messages is: {spam_skewness}')





from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
#nltk.download('stopwords')
#nltk.download('wordnet')
from nltk.stem import SnowballStemmer
import string


def text_preprocessing(text):
    #Lowercasing the text:
    text=text.lower()
    #Tokenizing the document into words:
    text=nltk.word_tokenize(text)
    #Removing Special characters:
    new_text=[]
    for word in text:
        if word.isalnum():
            new_text.append(word)
    text=new_text[:]
    new_text.clear()
    
    #Removing Stop Words:
    for word in text:
        if word not in stopwords.words('english') and word not in string.punctuation:
            new_text.append(word)

    text=new_text[:]
    new_text.clear()
    
    #Stemming:
    snow_ball=SnowballStemmer('english')
    for word in text:
        new_text.append(snow_ball.stem(word))

    return " ".join(new_text)


df1['Transformed_Text']=df1.Text.apply(text_preprocessing)
df1.sample(5)


from wordcloud import WordCloud
wc=WordCloud(width=500, height=500, min_font_size=10, background_color='black')


ham_words=df1[df1.Type==0]['Transformed_Text'].str.cat(sep=" ")


ham_wc=wc.generate(ham_words)


plt.imshow(ham_wc)
plt.title('Most frequent words in ham messages')
plt.show()


spam_words=df1[df1.Type==1]['Transformed_Text'].str.cat(sep=" ")


spam_wc=wc.generate(spam_words)


plt.imshow(spam_wc)
plt.title('Most frequent words in spam messages')
plt.show()


#!pip install collections
from collections import Counter


def text_counter(text,number=15):
    all_text=" ".join(text)
    all_word=word_tokenize(all_text)
    word_count=Counter(all_word)
    top_words=word_count.most_common(number)
    return top_words


ham_top_words=text_counter(df1[df1.Type==0]['Transformed_Text'])
ham_df = pd.DataFrame(ham_top_words, columns=['Word', 'Frequency'])


spam_top_words=text_counter(df1[df1.Type==1]['Transformed_Text'])
spam_df = pd.DataFrame(spam_top_words, columns=['Word', 'Frequency'])


fig, axes = plt.subplots(1, 2, figsize=(15, 6))
axes[0].bar(ham_df['Word'],ham_df['Frequency'], color='blue')
axes[0].set_title('Top 15 Ham Words')
axes[1].bar(spam_df['Word'],spam_df['Frequency'], color='red')
axes[1].set_title('Top 15 Spam Words')

plt.show()








from sklearn.feature_extraction.text import TfidfVectorizer
tf=TfidfVectorizer(max_features=3000)


X=tf.fit_transform(df1['Transformed_Text']).toarray()#to convert the sparse array into dense array
X.shape


Y=df1.Type.values
Y


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix


X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=42,test_size=0.2)


from sklearn.naive_bayes import MultinomialNB,BernoulliNB

mnb=MultinomialNB()
bnb=BernoulliNB()


mnb.fit(X_train,Y_train)
ypred1=mnb.predict(X_test)


print('Multinomial Naive Bayes:')
print('The accuracy score is:',accuracy_score(Y_test,ypred1))
print('The precision score is:',precision_score(Y_test,ypred1))


from sklearn import metrics
print('Confusion Matrix for Multinomial Naive Bayes:')
cm=confusion_matrix(Y_test,ypred1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Paired')
plt.show()


bnb.fit(X_train,Y_train)
ypred2=bnb.predict(X_test)


print('Bernoulli Naive Bayes:')
print('The accuracy score is:',accuracy_score(Y_test,ypred2))
print('The precision score is:',precision_score(Y_test,ypred2))


print('Confusion Matrix for Bernoulli Naive Bayes:')
cm=confusion_matrix(Y_test,ypred2)
sns.heatmap(cm, annot=True, fmt='d', cmap='Paired')
plt.show()





from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier


lr=LogisticRegression(penalty='l1')
svc=SVC()
knn=KNeighborsClassifier()
dtc=DecisionTreeClassifier(max_depth=5)
rfc=RandomForestClassifier(n_estimators=50, random_state=42)
gbc=GradientBoostingClassifier(n_estimators=50, random_state=42)


classifiers={
    "SVC":svc,
    "KNN":knn,
    "DT":dtc,
    "RF":rfc,
    "GB":gbc,
    "MNB":mnb,
    "BNB":bnb
}


def train_classifier(classifier,X_train,Y_train,X_test,Y_test):
    classifier.fit(X_train,Y_train)
    Y_pred=classifier.predict(X_test)
    
    accuracy=accuracy_score(Y_test,Y_pred)
    precision=precision_score(Y_test,Y_pred)

    #print(f' The accuracy of the model {classifier} is: {accuracy}')
    #print(f' The precision of the model {classifier} is: {precision}')

    return accuracy,precision


train_classifier(svc,X_train,Y_train,X_test,Y_test)


#from sklearn.metrics import accuracy_score, precision_score

accuracy_scores=[]
precision_scores=[]

for models,clf in classifiers.items():
    acc,pre=train_classifier(clf,X_train,Y_train,X_test,Y_test)
    accuracy_scores.append(acc)
    precision_scores.append(pre)


performance_dataframe=pd.DataFrame({'Algorithm':classifiers.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)


performance_dataframe





import pickle
pickle.dump(tf,open('vectorizer.pkl','wb'))
pickle.dump(bnb,open('model.pkl','wb'))
