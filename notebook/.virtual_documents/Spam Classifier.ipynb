import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pylab as plt


df=pd.read_csv('spam.csv',encoding="latin-1")


df1=df.copy()


df1.shape


df1.sample(5)


df1.info()





df1.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True,axis=1)


df1=df1.rename(columns={'v1':'Type','v2':'Text'})


from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()


df1.Type=le.fit_transform(df1.Type)


df1.sample(5)


df1.isnull().sum()


def delete_duplicate_values(df):
    print('Deleting the duplicated values:')
    num=df.duplicated().sum()
    if num>0:
        print(f'There are {num} duplicate values in your dataset.')
        df.drop_duplicates(keep='first',inplace=True)
        print(f'{num} duplicate values have been deleted.')
    else:
        print('There are no duplicate values.')


delete_duplicate_values(df1)





df1.Type.value_counts()


plt.pie(df1.Type.value_counts(),autopct='%0.2f',labels=['ham','spam'])
plt.title('Pie Chart of Type of text messages')
plt.show()





import nltk


#Finding the number of characters in the document:
df1['num_characters'] = df1['Text'].apply(len)


#nltk.download('punkt')
#nltk.download('punkt_tab')
df1['num_words']=df1.Text.apply(lambda x:len(nltk.word_tokenize(x)))


df1['num_sentences']=df1.Text.apply(lambda x:len(nltk.sent_tokenize(x)))


df1.sample(5)


df1[['num_characters','num_words','num_sentences']].describe().T





pd.set_option('display.max_colwidth', None)


df1[df1.num_words==220]





df1.drop(index=1578,inplace=True)


df1[['num_characters','num_words','num_sentences']].describe().T


df1[df1.num_words==196]





df1[df1.Type==0][['num_characters','num_words','num_sentences']].describe().T


df1[df1.Type==1][['num_characters','num_words','num_sentences']].describe().T





sns.histplot(df1[df1.Type==0]['num_characters'],color='green',kde=True,label='Not Spam')
sns.histplot(df1[df1.Type==1]['num_characters'],color='red',kde=True,label='Spam')
plt.legend(title='Message Type')
plt.title('Distribution of Number of Characters for Spam vs. Non-Spam Messages')
plt.show()


ham_skewness=df1[df1.Type==0]['num_characters'].skew()
spam_skewness=df1[df1.Type==1]['num_characters'].skew()


print(f' The skewness of characters in not spam messages is: {ham_skewness}')
print(f' The skewness of characters in spam messages is: {spam_skewness}')


sns.histplot(df1[df1.Type==0]['num_words'],color='purple',kde=True,label='Not Spam')
sns.histplot(df1[df1.Type==1]['num_words'],color='blue',kde=True,label='Spam')
plt.legend(title='Message Type')
plt.title('Distribution of Number of Words for Spam vs. Non-Spam Messages')
plt.show()


ham_skewness=df1[df1.Type==0]['num_words'].skew()
spam_skewness=df1[df1.Type==1]['num_words'].skew()


print(f' The skewness of words in not spam messages is:{ham_skewness}')
print(f' The skewness of words in spam messages is: {spam_skewness}')


sns.histplot(df1[df1.Type==0]['num_sentences'],color='yellow',kde=True,label='Not Spam')
sns.histplot(df1[df1.Type==1]['num_sentences'],color='black',kde=True,label='Spam')
plt.legend(title='Message Type')
plt.title('Distribution of Number of Sentences for Spam vs. Non-Spam Messages')
plt.show()


ham_skewness=df1[df1.Type==0]['num_sentences'].skew()
spam_skewness=df1[df1.Type==1]['num_sentences'].skew()


print(f' The skewness of sentences in not spam messages is: {ham_skewness}')
print(f' The skewness of sentences in spam messages is: {spam_skewness}')





from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
#nltk.download('stopwords')
#nltk.download('wordnet')
from nltk.stem import SnowballStemmer
import string


def text_preprocessing(text):
    #Lowercasing the text:
    text=text.lower()
    #Tokenizing the document into words:
    text=nltk.word_tokenize(text)
    #Removing Special characters:
    new_text=[]
    for word in text:
        if word.isalnum():
            new_text.append(word)
    text=new_text[:]
    new_text.clear()
    
    #Removing Stop Words:
    for word in text:
        if word not in stopwords.words('english') and word not in string.punctuation:
            new_text.append(word)

    text=new_text[:]
    new_text.clear()
    
    #Stemming:
    snow_ball=SnowballStemmer('english')
    for word in text:
        new_text.append(snow_ball.stem(word))

    return " ".join(new_text)


df1['Transformed_Text']=df1.Text.apply(text_preprocessing)
df1.sample(5)


from wordcloud import WordCloud
wc=WordCloud(width=500, height=500, min_font_size=10, background_color='black')


ham_words=df1[df1.Type==0]['Transformed_Text'].str.cat(sep=" ")


ham_wc=wc.generate(ham_words)


plt.imshow(ham_wc)
plt.title('Most frequent words in ham messages')
plt.show()


spam_words=df1[df1.Type==1]['Transformed_Text'].str.cat(sep=" ")


spam_wc=wc.generate(spam_words)


plt.imshow(spam_wc)
plt.title('Most frequent words in spam messages')
plt.show()


#!pip install collections
from collections import Counter


def text_counter(text,number=15):
    all_text=" ".join(text)
    all_word=word_tokenize(all_text)
    word_count=Counter(all_word)
    top_words=word_count.most_common(number)
    return top_words


ham_top_words=text_counter(df1[df1.Type==0]['Transformed_Text'])
ham_df = pd.DataFrame(ham_top_words, columns=['Word', 'Frequency'])


spam_top_words=text_counter(df1[df1.Type==1]['Transformed_Text'])
spam_df = pd.DataFrame(spam_top_words, columns=['Word', 'Frequency'])


fig, axes = plt.subplots(1, 2, figsize=(15, 6))
axes[0].bar(ham_df['Word'],ham_df['Frequency'], color='blue')
axes[0].set_title('Top 15 Ham Words')
axes[1].bar(spam_df['Word'],spam_df['Frequency'], color='red')
axes[1].set_title('Top 15 Spam Words')

plt.show()






